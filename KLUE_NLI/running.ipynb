{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"o-XUJ2EvrnaX"},"outputs":[],"source":["!pip install transformers\n","!pip install datasets\n","!pip install wandb\n","!pip install python-dotenv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r41kPzezrVJj"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zimt39PlrD39"},"outputs":[],"source":["%cd /content/drive/Othercomputers/내 컴퓨터/KLUE_NLI"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yk_qI8ksrIwR"},"outputs":[],"source":["%pwd"]},{"cell_type":"markdown","metadata":{},"source":["# Train & Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g5wjWqbmRRq5"},"outputs":[],"source":["# model 1 : valid_rtt + Roberta-Large + R-drop\n","!python train.py \\\n","--data_name train_data.csv \\\n","--valid_rtt True \\\n","--k_fold 5 \\\n","--run_name Roberta_Large_rdrop \\\n","--save_path ./checkpoints/Roberta_Large_rdrop \\\n","--model_name_or_path klue/roberta-large \\\n","--do_train \\\n","--do_eval \\\n","--output_dir /content/results \\\n","--overwrite_output_dir True \\\n","--save_total_limit 5 \\\n","--save_strategy steps \\\n","--num_train_epochs 2 \\\n","--learning_rate 3e-5 \\\n","--per_device_train_batch_size 16 \\\n","--per_device_eval_batch_size 32 \\\n","--gradient_accumulation_steps 1 \\\n","--evaluation_strategy steps \\\n","--logging_steps 100 \\\n","--eval_steps 500 \\\n","--save_steps 500 \\\n","--fp16 True \\\n","--load_best_model_at_end True \\\n","--metric_for_best_model accuracy \\\n","--warmup_ratio 0.1 \\\n","--weight_decay 1e-3 \\\n","--use_rdrop True "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python inference.py \\\n","--output_dir ./checkpoints \\\n","--overwrite_output_dir True \\\n","--k_fold 5 \\\n","--model_name_or_path klue/roberta-large \\\n","--save_path ./checkpoints/Roberta_Large_rdrop \\\n","--output_name Roberta_Large_rdrop.csv \\\n","--per_device_eval_batch_size 32 \\\n","--do_predict True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model 2 : valid_rtt + Roberta-LSTM + R-drop\n","!python train.py \\\n","--data_name train_data.csv \\\n","--valid_rtt True \\\n","--k_fold 5 \\\n","--run_name Roberta_LSTM_rdrop \\\n","--save_path ./checkpoints/Roberta_LSTM_rdrop \\\n","--model_name_or_path klue/roberta-large \\\n","--do_train \\\n","--do_eval \\\n","--output_dir /content/results \\\n","--overwrite_output_dir True \\\n","--save_total_limit 5 \\\n","--save_strategy steps \\\n","--num_train_epochs 3 \\\n","--learning_rate 3e-5 \\\n","--per_device_train_batch_size 16 \\\n","--per_device_eval_batch_size 32 \\\n","--gradient_accumulation_steps 2 \\\n","--evaluation_strategy steps \\\n","--logging_steps 100 \\\n","--eval_steps 500 \\\n","--save_steps 500 \\\n","--fp16 True \\\n","--load_best_model_at_end True \\\n","--metric_for_best_model accuracy \\\n","--warmup_ratio 0.1 \\\n","--weight_decay 1e-3 \\\n","--use_lstm True \\\n","--use_rdrop True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python inference.py \\\n","--output_dir ./checkpoints \\\n","--overwrite_output_dir True \\\n","--k_fold 5 \\\n","--model_name_or_path klue/roberta-large \\\n","--save_path ./checkpoints/Roberta_LSTM_rdrop \\\n","--output_name Roberta_LSTM_rdrop.csv \\\n","--per_device_eval_batch_size 32 \\\n","--do_predict True \\\n","--use_lstm True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model 3 : valid_rtt + Self-Explainable + R-drop\n","!python train.py \\\n","--data_name train_data.csv \\\n","--aeda True \\\n","--valid_rtt True \\\n","--k_fold 5 \\\n","--run_name Explainable_3ep_rdrop \\\n","--save_path ./checkpoints/Explainable_3ep_rdrop \\\n","--model_name_or_path klue/roberta-large \\\n","--do_train \\\n","--do_eval \\\n","--output_dir /content/results \\\n","--overwrite_output_dir True \\\n","--save_total_limit 5 \\\n","--save_strategy steps \\\n","--num_train_epochs 3 \\\n","--learning_rate 1e-5 \\\n","--per_device_train_batch_size 10 \\\n","--per_device_eval_batch_size 32 \\\n","--gradient_accumulation_steps 1 \\\n","--evaluation_strategy steps \\\n","--logging_steps 100 \\\n","--eval_steps 500 \\\n","--save_steps 500 \\\n","--fp16 True \\\n","--load_best_model_at_end True \\\n","--metric_for_best_model accuracy \\\n","--warmup_ratio 0.1 \\\n","--weight_decay 1e-3 \\\n","--use_SIC True \\\n","--use_rdrop True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python inference.py \\\n","--output_dir ./checkpoints \\\n","--overwrite_output_dir True \\\n","--k_fold 5 \\\n","--model_name_or_path klue/roberta-large \\\n","--save_path ./checkpoints/Explainable_3ep_rdrop \\\n","--output_name Explainable_3ep_rdrop.csv \\\n","--per_device_eval_batch_size 32 \\\n","--do_predict True \\\n","--use_SIC True "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model 4 : Aeda data + Self-Explainable\n","!python train.py \\\n","--data_name train_data.csv \\\n","--aeda True \\\n","--k_fold 5 \\\n","--run_name Explainable_aeda  \\\n","--save_path ./checkpoints/Explainable_aeda  \\\n","--model_name_or_path klue/roberta-large \\\n","--do_train \\\n","--do_eval \\\n","--output_dir /content/results \\\n","--overwrite_output_dir True \\\n","--save_total_limit 5 \\\n","--save_strategy steps \\\n","--num_train_epochs 3 \\\n","--learning_rate 5e-5 \\\n","--per_device_train_batch_size 16 \\\n","--per_device_eval_batch_size 32 \\\n","--gradient_accumulation_steps 2 \\\n","--evaluation_strategy steps \\\n","--logging_steps 100 \\\n","--eval_steps 500 \\\n","--save_steps 500 \\\n","--fp16 True \\\n","--load_best_model_at_end True \\\n","--metric_for_best_model accuracy \\\n","--label_smoothing_factor 0.1 \\\n","--warmup_ratio 0.1 \\\n","--weight_decay 1e-2 \\\n","--use_SIC True "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGaZeA5vZ_1m"},"outputs":[],"source":["!python inference.py \\\n","--output_dir ./checkpoints \\\n","--overwrite_output_dir True \\\n","--k_fold 5 \\\n","--model_name_or_path klue/roberta-large \\\n","--save_path ./checkpoints/Explainable_aeda \\\n","--output_name Explainable_aeda.csv \\\n","--per_device_eval_batch_size 32 \\\n","--do_predict True \\\n","--use_SIC True "]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO0j8aywNeM8OTihHqL+3Hs","collapsed_sections":[],"mount_file_id":"1cnjm4_WqK4sbyCZHFquu1-HrzbSnx2OH","name":"running.ipynb","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.11"}},"nbformat":4,"nbformat_minor":0}
